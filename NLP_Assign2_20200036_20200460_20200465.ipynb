{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5737caa2",
      "metadata": {
        "id": "5737caa2"
      },
      "source": [
        "# Ahmed Mohamed Ahemd 20200036\n",
        "# Mohamed Abd ElGhaffar  20200460\n",
        "# Mohamed Essam Galal     20200465"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3762b6",
      "metadata": {
        "id": "8a3762b6"
      },
      "source": [
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b3615c",
      "metadata": {
        "id": "04b3615c"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "84cb0e38",
      "metadata": {
        "id": "84cb0e38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nltk.tokenize import  word_tokenize\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kJsOd0Ze1x-",
        "outputId": "7b19694e-4ca2-4f87-8aeb-c09b63989c1c"
      },
      "id": "-kJsOd0Ze1x-",
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d6d8465",
      "metadata": {
        "id": "4d6d8465"
      },
      "source": [
        "## Tokenization | Stemming | Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "4c7cad5a",
      "metadata": {
        "id": "4c7cad5a"
      },
      "outputs": [],
      "source": [
        "def tokenize(preprocessed_words):\n",
        "    return nltk.word_tokenize(preprocessed_words)\n",
        "\n",
        "def stemming(word):\n",
        "    stemmer = PorterStemmer()\n",
        "    return stemmer.stem(word)\n",
        "\n",
        "def lemmatizing(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return lemmatizer.lemmatize(word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb213258",
      "metadata": {
        "id": "fb213258"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "4b598334",
      "metadata": {
        "id": "4b598334"
      },
      "outputs": [],
      "source": [
        "def preprocessing(text):\n",
        "    preprocessed_text = re.sub(r'\\s+', ' ', text)                            # Remove All WhiteSpaces\n",
        "    preprocessed_text = re.sub(r'[^a-zA-Z0-9\\s]', '',preprocessed_text)      # Remove All Charchters Excepts Letters and Numbers\n",
        "    norm_preprocessed_text = preprocessed_text.lower()                       # Lower All Words \"Normalization\"\n",
        "    tokens = tokenize(norm_preprocessed_text)                                # Tokenizzing\n",
        "\n",
        "    stop_words = stopwords.words('english')                                  # Stop-Words to be Removed\n",
        "\n",
        "    stem_words = [stemming(word) for word in tokens if word not in stop_words] # Stemming\n",
        "\n",
        "    lemmatize_words = [lemmatizing(word) for word in tokens if word not in stop_words] # Lemmatizing\n",
        "\n",
        "    return lemmatize_words\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "623e0b5b",
      "metadata": {
        "id": "623e0b5b"
      },
      "source": [
        "## Getting Unique Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "049cd38d",
      "metadata": {
        "id": "049cd38d"
      },
      "outputs": [],
      "source": [
        "def unique_words(stem_words = None, lemmatize_words = None):\n",
        "    try:\n",
        "        return set(lemmatize_words), set(stem_words)\n",
        "    except Exception as e:\n",
        "        print(\"Error:\", e)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee5956c7",
      "metadata": {
        "id": "ee5956c7"
      },
      "source": [
        "## Getting Categories \"Fields\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "18e1bbba",
      "metadata": {
        "id": "18e1bbba"
      },
      "outputs": [],
      "source": [
        "# def select_random_categories(phrases):\n",
        "#     brown_categories = [\n",
        "#         'news',\n",
        "#         'editorial',\n",
        "#         'reviews',\n",
        "#         'religion',\n",
        "#         'hobbies',\n",
        "#         'lore',\n",
        "#         'belles_lettres',\n",
        "#         'government',\n",
        "#         'learned',\n",
        "#         'fiction',\n",
        "#         'mystery',\n",
        "#         'science_fiction',\n",
        "#         'adventure',\n",
        "#         'romance',\n",
        "#         'humor'\n",
        "#     ]\n",
        "\n",
        "#     num_categories = min(phrases, len(brown_categories))\n",
        "\n",
        "#     random.shuffle(brown_categories)\n",
        "\n",
        "#     return brown_categories[:num_categories]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1601363",
      "metadata": {
        "id": "f1601363"
      },
      "source": [
        "## Generate Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "d83549ba",
      "metadata": {
        "id": "d83549ba"
      },
      "outputs": [],
      "source": [
        "generator = pipeline('text-generation', model='gpt2')\n",
        "# differentÂ field\n",
        "categories = [\"news\",\"editorial\",\"reviews\",\"religion\",\"hobbies\"]\n",
        "def generate_documents():\n",
        "    documents = []\n",
        "    for category in categories:\n",
        "        generated_text = generator(category, max_length=100)[0]['generated_text']\n",
        "        documents.append(generated_text)\n",
        "\n",
        "    return documents\n",
        "\n",
        "# def generate_documents(phrases):\n",
        "#     categories = select_random_categories(phrases)\n",
        "#     documents = []\n",
        "#     for category in categories:\n",
        "#         brown_corpus = brown.sents(categories=[category])\n",
        "#         documents.append(' '.join(brown_corpus[1]))\n",
        "\n",
        "#     return documents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8476c2fc",
      "metadata": {
        "id": "8476c2fc"
      },
      "source": [
        "## tfidf_vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "835b12d6",
      "metadata": {
        "id": "835b12d6"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac6aabe",
      "metadata": {
        "id": "dac6aabe"
      },
      "source": [
        "## Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "10467b41",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10467b41",
        "outputId": "790ae3ac-9a75-440f-c1ec-8b76df97ee59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"news are a big part of the public perception of what's being done with the NSA. When they're not trying to protect your rights, they're trying to intimidate you to do the right thing. That's how they use you to get what they want.\\n\\nAnd they're doing it to have you think a bigger deal about what's happening with the NSA, in every other area of public policy. If you ask me, how do you have Congress, the president, on your side,\",\n",
              " 'editorial that had an interest in sex in the 1980s.\\n\\nKonstantin is now a consultant in the department of political science at the University of Stirling. \"I really had an interest in sex when I was in Stirling when I did a book on politics, I went to see it and I was amazed by how little interest that it actually drew,\" he said.\\n\\nThe former Liberal MP described it as an important read, even if the book has its downsides',\n",
              " 'reviews\\n\\nThis was my first foray into the post-modern world of writing, being an avid fan since I was a little kid. I have come out and joined the literary scene as a result and I have spent my life as an avid reader, enjoying novels for many years.\\n\\nThe first thing to notice about this book was its focus on style, its story and the way it was meant to be read. For a book which has become a daily staple for me, my first',\n",
              " 'religion, the Bible, and Jewish morality, in which the human person has a natural capacity to love God, is simply not understood into actual reality. Many modern philosophers of the day take this position. According to Thomas Aquinas, human nature is, in fact, a natural force which has arisen on a single spot on the face of the earth, and that which is not a part of being is not subject to natural selection. Aquinas states, however, that an act of natural selection may',\n",
              " 'hobbies. While we will never know for certain, you will always be remembered for whatever you were able to do at a moment\\'s notice. But I wanted to create a new experience, one that would delight the entire human family.\"\\n\\nAs soon as he finished, he opened his mouth \"So, this is one and only: An Old Dog who had been with me for years... I had the misfortune to take away his life, as well as his wife and children.\"\\n\\nThe']"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ],
      "source": [
        "documents = generate_documents()\n",
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "56c1b94e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56c1b94e",
        "outputId": "a65942cc-8e62-4bc7-e4f8-d8989f6d3e41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<5x154 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 165 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "id": "0beeceb6",
      "metadata": {
        "id": "0beeceb6"
      },
      "outputs": [],
      "source": [
        "process_docs = []\n",
        "for document in documents:\n",
        "    doc = preprocessing(document)\n",
        "    process_docs.append(doc)\n",
        "\n",
        "allDocuments = []\n",
        "word_set = []\n",
        "\n",
        "for document in process_docs:\n",
        "    allDocuments.append(document)\n",
        "    for word in document:\n",
        "        word_set.append(word)\n",
        "\n",
        "word_set = set(word_set)\n",
        "N_documents = len(allDocuments)\n",
        "\n",
        "index_word = {}\n",
        "i = 0\n",
        "for word in word_set:\n",
        "    index_word[word] = i\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def TF(document, word):\n",
        "    return document.count(word) / len(document)"
      ],
      "metadata": {
        "id": "I9V_3iAx3pjL"
      },
      "id": "I9V_3iAx3pjL",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def IDF(word, allDocuments):\n",
        "    num_documents_with_word = sum([1 for doc in allDocuments if word in doc])\n",
        "    return np.log((N_documents + 1)/(num_documents_with_word + 1)) + 1"
      ],
      "metadata": {
        "id": "zqiQtfqh3plt"
      },
      "id": "zqiQtfqh3plt",
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TF_IDF(document):\n",
        "    TF_IDF_Vec = np.zeros((len(word_set),))\n",
        "    for word in document:\n",
        "        tf = TF(document,word)\n",
        "        idf = IDF(word, allDocuments)\n",
        "\n",
        "        value = tf*idf\n",
        "        TF_IDF_Vec[index_word[word]] = value\n",
        "\n",
        "    return TF_IDF_Vec"
      ],
      "metadata": {
        "id": "M1GjmAqm3po4"
      },
      "id": "M1GjmAqm3po4",
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectors = []\n",
        "for document in allDocuments:\n",
        "    vec = TF_IDF(document)\n",
        "    vectors.append(vec)"
      ],
      "metadata": {
        "id": "2tsQNC6B3psD"
      },
      "id": "2tsQNC6B3psD",
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "id": "b0f5ab8e",
      "metadata": {
        "id": "b0f5ab8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa47945e-ec23-4382-d669-d5cd98bb8599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF from Sklearn:\n",
            "Document 1:\n",
            "      area: 0.1396\n",
            "      ask: 0.1396\n",
            "      big: 0.1396\n",
            "      bigger: 0.1396\n",
            "      congress: 0.1396\n",
            "      deal: 0.1396\n",
            "      done: 0.1396\n",
            "      every: 0.1396\n",
            "      get: 0.1396\n",
            "      happening: 0.1396\n",
            "      intimidate: 0.1396\n",
            "      news: 0.1396\n",
            "      nsa: 0.2792\n",
            "      part: 0.1126\n",
            "      perception: 0.1396\n",
            "      policy: 0.1396\n",
            "      president: 0.1396\n",
            "      protect: 0.1396\n",
            "      public: 0.2792\n",
            "      right: 0.2792\n",
            "      side: 0.1396\n",
            "      thats: 0.1396\n",
            "      theyre: 0.4188\n",
            "      thing: 0.1126\n",
            "      think: 0.1396\n",
            "      trying: 0.2792\n",
            "      use: 0.1396\n",
            "      want: 0.1396\n",
            "      whats: 0.2792\n",
            "\n",
            "Document 2:\n",
            "      1980s: 0.1509\n",
            "      actually: 0.1509\n",
            "      amazed: 0.1509\n",
            "      book: 0.2435\n",
            "      consultant: 0.1509\n",
            "      department: 0.1509\n",
            "      described: 0.1509\n",
            "      downside: 0.1509\n",
            "      drew: 0.1509\n",
            "      editorial: 0.1509\n",
            "      even: 0.1509\n",
            "      former: 0.1509\n",
            "      important: 0.1509\n",
            "      interest: 0.4528\n",
            "      konstantin: 0.1509\n",
            "      liberal: 0.1509\n",
            "      little: 0.1218\n",
            "      mp: 0.1509\n",
            "      political: 0.1509\n",
            "      politics: 0.1509\n",
            "      read: 0.1218\n",
            "      really: 0.1509\n",
            "      said: 0.1509\n",
            "      science: 0.1509\n",
            "      see: 0.1509\n",
            "      sex: 0.3018\n",
            "      stirling: 0.3018\n",
            "      university: 0.1509\n",
            "      went: 0.1509\n",
            "\n",
            "Document 3:\n",
            "      avid: 0.2976\n",
            "      become: 0.1488\n",
            "      book: 0.2401\n",
            "      come: 0.1488\n",
            "      daily: 0.1488\n",
            "      enjoying: 0.1488\n",
            "      fan: 0.1488\n",
            "      first: 0.4464\n",
            "      focus: 0.1488\n",
            "      foray: 0.1488\n",
            "      joined: 0.1488\n",
            "      kid: 0.1488\n",
            "      life: 0.1201\n",
            "      literary: 0.1488\n",
            "      little: 0.1201\n",
            "      many: 0.1201\n",
            "      meant: 0.1488\n",
            "      notice: 0.1201\n",
            "      novel: 0.1488\n",
            "      postmodern: 0.1488\n",
            "      read: 0.1201\n",
            "      reader: 0.1488\n",
            "      result: 0.1488\n",
            "      review: 0.1488\n",
            "      scene: 0.1488\n",
            "      since: 0.1488\n",
            "      spent: 0.1488\n",
            "      staple: 0.1488\n",
            "      story: 0.1488\n",
            "      style: 0.1488\n",
            "      thing: 0.1201\n",
            "      way: 0.1488\n",
            "      world: 0.1488\n",
            "      writing: 0.1488\n",
            "      year: 0.1201\n",
            "\n",
            "Document 4:\n",
            "      according: 0.1296\n",
            "      act: 0.1296\n",
            "      actual: 0.1296\n",
            "      aquinas: 0.2592\n",
            "      arisen: 0.1296\n",
            "      bible: 0.1296\n",
            "      capacity: 0.1296\n",
            "      day: 0.1296\n",
            "      earth: 0.1296\n",
            "      face: 0.1296\n",
            "      fact: 0.1296\n",
            "      force: 0.1296\n",
            "      god: 0.1296\n",
            "      however: 0.1296\n",
            "      human: 0.2091\n",
            "      jewish: 0.1296\n",
            "      love: 0.1296\n",
            "      many: 0.1045\n",
            "      may: 0.1296\n",
            "      modern: 0.1296\n",
            "      morality: 0.1296\n",
            "      natural: 0.5183\n",
            "      nature: 0.1296\n",
            "      part: 0.1045\n",
            "      person: 0.1296\n",
            "      philosopher: 0.1296\n",
            "      position: 0.1296\n",
            "      reality: 0.1296\n",
            "      religion: 0.1296\n",
            "      selection: 0.2592\n",
            "      simply: 0.1296\n",
            "      single: 0.1296\n",
            "      spot: 0.1296\n",
            "      state: 0.1296\n",
            "      subject: 0.1296\n",
            "      take: 0.1045\n",
            "      thomas: 0.1296\n",
            "      understood: 0.1296\n",
            "\n",
            "Document 5:\n",
            "      able: 0.1684\n",
            "      always: 0.1684\n",
            "      away: 0.1684\n",
            "      certain: 0.1684\n",
            "      child: 0.1684\n",
            "      create: 0.1684\n",
            "      delight: 0.1684\n",
            "      dog: 0.1684\n",
            "      entire: 0.1684\n",
            "      experience: 0.1684\n",
            "      family: 0.1684\n",
            "      finished: 0.1684\n",
            "      hobby: 0.1684\n",
            "      human: 0.1359\n",
            "      know: 0.1684\n",
            "      life: 0.1359\n",
            "      misfortune: 0.1684\n",
            "      moment: 0.1684\n",
            "      mouth: 0.1684\n",
            "      never: 0.1684\n",
            "      new: 0.1684\n",
            "      notice: 0.1359\n",
            "      old: 0.1684\n",
            "      one: 0.3368\n",
            "      opened: 0.1684\n",
            "      remembered: 0.1684\n",
            "      soon: 0.1684\n",
            "      take: 0.1359\n",
            "      wanted: 0.1684\n",
            "      well: 0.1684\n",
            "      whatever: 0.1684\n",
            "      wife: 0.1684\n",
            "      would: 0.1684\n",
            "      year: 0.1359\n",
            "\n",
            "TF-IDF from Scratch:\n",
            "Document 1:\n",
            "      area: 0.1396\n",
            "      ask: 0.1396\n",
            "      big: 0.1396\n",
            "      bigger: 0.1396\n",
            "      congress: 0.1396\n",
            "      deal: 0.1396\n",
            "      done: 0.1396\n",
            "      every: 0.1396\n",
            "      get: 0.1396\n",
            "      happening: 0.1396\n",
            "      intimidate: 0.1396\n",
            "      news: 0.1396\n",
            "      nsa: 0.2792\n",
            "      part: 0.1126\n",
            "      perception: 0.1396\n",
            "      policy: 0.1396\n",
            "      president: 0.1396\n",
            "      protect: 0.1396\n",
            "      public: 0.2792\n",
            "      right: 0.2792\n",
            "      side: 0.1396\n",
            "      thats: 0.1396\n",
            "      theyre: 0.4188\n",
            "      thing: 0.1126\n",
            "      think: 0.1396\n",
            "      trying: 0.2792\n",
            "      use: 0.1396\n",
            "      want: 0.1396\n",
            "      whats: 0.2792\n",
            "\n",
            "Document 2:\n",
            "      1980s: 0.1509\n",
            "      actually: 0.1509\n",
            "      amazed: 0.1509\n",
            "      book: 0.2435\n",
            "      consultant: 0.1509\n",
            "      department: 0.1509\n",
            "      described: 0.1509\n",
            "      downside: 0.1509\n",
            "      drew: 0.1509\n",
            "      editorial: 0.1509\n",
            "      even: 0.1509\n",
            "      former: 0.1509\n",
            "      important: 0.1509\n",
            "      interest: 0.4528\n",
            "      konstantin: 0.1509\n",
            "      liberal: 0.1509\n",
            "      little: 0.1218\n",
            "      mp: 0.1509\n",
            "      political: 0.1509\n",
            "      politics: 0.1509\n",
            "      read: 0.1218\n",
            "      really: 0.1509\n",
            "      said: 0.1509\n",
            "      science: 0.1509\n",
            "      see: 0.1509\n",
            "      sex: 0.3018\n",
            "      stirling: 0.3018\n",
            "      university: 0.1509\n",
            "      went: 0.1509\n",
            "\n",
            "Document 3:\n",
            "      avid: 0.2976\n",
            "      become: 0.1488\n",
            "      book: 0.2401\n",
            "      come: 0.1488\n",
            "      daily: 0.1488\n",
            "      enjoying: 0.1488\n",
            "      fan: 0.1488\n",
            "      first: 0.4464\n",
            "      focus: 0.1488\n",
            "      foray: 0.1488\n",
            "      joined: 0.1488\n",
            "      kid: 0.1488\n",
            "      life: 0.1201\n",
            "      literary: 0.1488\n",
            "      little: 0.1201\n",
            "      many: 0.1201\n",
            "      meant: 0.1488\n",
            "      notice: 0.1201\n",
            "      novel: 0.1488\n",
            "      postmodern: 0.1488\n",
            "      read: 0.1201\n",
            "      reader: 0.1488\n",
            "      result: 0.1488\n",
            "      review: 0.1488\n",
            "      scene: 0.1488\n",
            "      since: 0.1488\n",
            "      spent: 0.1488\n",
            "      staple: 0.1488\n",
            "      story: 0.1488\n",
            "      style: 0.1488\n",
            "      thing: 0.1201\n",
            "      way: 0.1488\n",
            "      world: 0.1488\n",
            "      writing: 0.1488\n",
            "      year: 0.1201\n",
            "\n",
            "Document 4:\n",
            "      according: 0.1296\n",
            "      act: 0.1296\n",
            "      actual: 0.1296\n",
            "      aquinas: 0.2592\n",
            "      arisen: 0.1296\n",
            "      bible: 0.1296\n",
            "      capacity: 0.1296\n",
            "      day: 0.1296\n",
            "      earth: 0.1296\n",
            "      face: 0.1296\n",
            "      fact: 0.1296\n",
            "      force: 0.1296\n",
            "      god: 0.1296\n",
            "      however: 0.1296\n",
            "      human: 0.2091\n",
            "      jewish: 0.1296\n",
            "      love: 0.1296\n",
            "      many: 0.1045\n",
            "      may: 0.1296\n",
            "      modern: 0.1296\n",
            "      morality: 0.1296\n",
            "      natural: 0.5183\n",
            "      nature: 0.1296\n",
            "      part: 0.1045\n",
            "      person: 0.1296\n",
            "      philosopher: 0.1296\n",
            "      position: 0.1296\n",
            "      reality: 0.1296\n",
            "      religion: 0.1296\n",
            "      selection: 0.2592\n",
            "      simply: 0.1296\n",
            "      single: 0.1296\n",
            "      spot: 0.1296\n",
            "      state: 0.1296\n",
            "      subject: 0.1296\n",
            "      take: 0.1045\n",
            "      thomas: 0.1296\n",
            "      understood: 0.1296\n",
            "\n",
            "Document 5:\n",
            "      able: 0.1684\n",
            "      always: 0.1684\n",
            "      away: 0.1684\n",
            "      certain: 0.1684\n",
            "      child: 0.1684\n",
            "      create: 0.1684\n",
            "      delight: 0.1684\n",
            "      dog: 0.1684\n",
            "      entire: 0.1684\n",
            "      experience: 0.1684\n",
            "      family: 0.1684\n",
            "      finished: 0.1684\n",
            "      hobby: 0.1684\n",
            "      human: 0.1359\n",
            "      know: 0.1684\n",
            "      life: 0.1359\n",
            "      misfortune: 0.1684\n",
            "      moment: 0.1684\n",
            "      mouth: 0.1684\n",
            "      never: 0.1684\n",
            "      new: 0.1684\n",
            "      notice: 0.1359\n",
            "      old: 0.1684\n",
            "      one: 0.3368\n",
            "      opened: 0.1684\n",
            "      remembered: 0.1684\n",
            "      soon: 0.1684\n",
            "      take: 0.1359\n",
            "      wanted: 0.1684\n",
            "      well: 0.1684\n",
            "      whatever: 0.1684\n",
            "      wife: 0.1684\n",
            "      would: 0.1684\n",
            "      year: 0.1359\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Normalization of TF-IDF vectors\n",
        "TF_IDF_Vec_Normalize = np.array(vectors) / np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "\n",
        "print(\"\\nTF-IDF from Sklearn:\")\n",
        "i = 0\n",
        "for document in documents:\n",
        "    print(f\"Document {i+1}:\")\n",
        "    unique_words = tfidf_vectorizer.get_feature_names_out()\n",
        "    j = 0\n",
        "    for word in unique_words:\n",
        "        tfidf = tfidf_matrix[i, j]\n",
        "        if tfidf > 0:\n",
        "            print(f\"      {word}: {tfidf:.4f}\")\n",
        "        j += 1\n",
        "    print(\"\")\n",
        "    i += 1\n",
        "\n",
        "print(\"TF-IDF from Scratch:\")\n",
        "i = 0\n",
        "for document in allDocuments:\n",
        "    arr = []\n",
        "    print(f\"Document {i+1}:\")\n",
        "    for j, feature in enumerate(word_set):\n",
        "        if vectors[i][j] > 0.0:\n",
        "            arr.append([feature, TF_IDF_Vec_Normalize[i][j]])\n",
        "    arr = sorted(arr, key=lambda x: x[0])\n",
        "    for item in arr:\n",
        "        print(f\"      {item[0]}: {item[1]:.4f}\")\n",
        "    print(\"\")\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}